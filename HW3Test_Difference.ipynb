{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 25]\n",
      "0.16844867497713667\n",
      "0.7468060255179592\n"
     ]
    }
   ],
   "source": [
    "#ROB538 HW3\n",
    "\n",
    "#Problem 1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self,K):\n",
    "        self.g = 0\n",
    "        self.states = []  # record position and action taken at the position\n",
    "        self.actions = list(range(0,K)) #Set list of days as actions\n",
    "        self.lr = 0.2 #learning rate, takes longer when low\n",
    "        self.exp_rate = 0.0 #exploration rate .7\n",
    "        self.decay_gamma = 0.5 #Reward decay does not work well when 1 .85\n",
    "\n",
    "        self.switch = False\n",
    "\n",
    "        # initial Q values\n",
    "        self.Q_values = list(range(0,K))\n",
    "        for a in self.actions:\n",
    "            self.Q_values[a] = 0  # Q value is a dict of dict\n",
    "\n",
    "    def giveReward(self,x_k,b,x_k_z):\n",
    "        self.indiv = x_k*np.exp(-x_k/b)\n",
    "        \n",
    "        self.G = 0 \n",
    "        for x_k_g in x_k_z:\n",
    "            self.G = x_k_g*np.exp(-x_k_g/b) +self.G\n",
    "        \n",
    "        #Add a difference reward\n",
    "        \n",
    "        #Calculate reward if agent didnt exist\n",
    "        self.minusi = 0\n",
    "        self.diff = 0\n",
    "        #Get list for if agent didnt exist\n",
    "        x_k_2 = x_k_z\n",
    "        x_k_2[self.action] = x_k_2[self.action] - 1\n",
    "        #Repeat reward calculation\n",
    "        for x_k_g2 in x_k_2:\n",
    "            self.minusi = x_k_g2*np.exp(-x_k_g2/b) + self.minusi\n",
    "        #Calculate difference reward\n",
    "        self.diff = self.G-self.minusi\n",
    "\n",
    "\n",
    "\n",
    "        return self.indiv,self.G,self.diff\n",
    "\n",
    "    def State(self):\n",
    "        pass\n",
    "\n",
    "    def chooseAction(self):\n",
    "        # choose action with most expected value\n",
    "        mx_nxt_reward = 0\n",
    "        action = \"\"\n",
    "\n",
    "        #Exploration action\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # greedy action\n",
    "            for a in self.actions:\n",
    "                nxt_reward = self.Q_values[a]\n",
    "                if nxt_reward >= mx_nxt_reward:\n",
    "                    action = a\n",
    "                    mx_nxt_reward = nxt_reward\n",
    "                #else:\n",
    "                    #action = np.random.choice(self.actions)\n",
    "            # print(\"current pos: {}, greedy aciton: {}\".format(self.State.state, action))\n",
    "        return action\n",
    "    \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.reward = 0\n",
    "        self.switch = False\n",
    "    \n",
    "    \n",
    "    def play(self, i,x_k,b,x_k_z):\n",
    "        if not self.switch:\n",
    "            # to the end of game back propagate reward\n",
    "            self.action = self.chooseAction()\n",
    "            # append trace\n",
    "            self.states.append(self.action) #Choose 1-6\n",
    "            #print(\"current position {} action {}\".format(self.State.state, action))\n",
    "            # mark is end\n",
    "            #self.State.isEndFunc()\n",
    "            self.switch = True\n",
    "            return(i,self)\n",
    "        else:\n",
    "        # back propagate\n",
    "            reward1,reward,reward3 = self.giveReward(x_k,b,x_k_z)\n",
    "            self.Q_values[self.action] = reward\n",
    "            for s in reversed(self.states):\n",
    "                current_q_value = self.Q_values\n",
    "                reward = current_q_value + self.lr * (self.decay_gamma * reward - current_q_value)\n",
    "                self.Q_values = reward\n",
    "            self.reset()\n",
    "            i += 1\n",
    "            return(i,self)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #initialize agents\n",
    "    N = 25\n",
    "    Agent_list = []\n",
    "    Action_list = [0]*N\n",
    "    \n",
    "    b = 5\n",
    "    k = 7\n",
    "    x_k = [0]*N\n",
    "    Count_list = [0]*k\n",
    "    Decision = [0]*k\n",
    "    #Create list of agent objects, each initialized\n",
    "    for i in range(N):\n",
    "        Agent_list.append(Agent(k))\n",
    "\n",
    "    #print(Agent_list[8].g) gives g\n",
    "    #Agent_list[8].indiv_reward(8,5)\n",
    "    #print(Agent_list[8].indiv)\n",
    "    i = np.zeros(N)\n",
    "    \n",
    "    while i[0]<300:\n",
    "        for x in range(N):\n",
    "            i[x],Agent_list[x] = Agent_list[x].play(i[x],x_k[x],b,Count_list)\n",
    "            #Get info on actions here\n",
    "            Action_list[x] = Agent_list[x].action\n",
    "            #Count up all\n",
    "            Count_list[Action_list[x]] = Count_list[Action_list[x]] + 1\n",
    "        for y in range(N):\n",
    "            #Set all x_k values and run again \n",
    "            x_k[y] = Count_list[Action_list[y]] #Number of people who attended same day\n",
    "        for z in range(N):\n",
    "            #Now distribute rewards\n",
    "            i[z],Agent_list[z] = Agent_list[z].play(i[z],x_k[z],b,Count_list)\n",
    "        #Reset Count List?\n",
    "        Count_list = [0]*k\n",
    "    #print(Agent_list[0].Q_values)\n",
    "    for i in range(N):\n",
    "        #For each agent\n",
    "        #Get the index of the maximum q_value\n",
    "        index = np.argmax(Agent_list[i].Q_values)\n",
    "        Decision[index] = Decision[index] + 1 \n",
    "\n",
    "    print(Decision) \n",
    "    print(Agent_list[0].G)\n",
    "    print(Agent_list[10].G)\n",
    "    #print(Agent_list[0].Q_values)\n",
    "    #print(Agent_list[0].action)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
