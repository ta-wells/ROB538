{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROB538 HW3\n",
    "\n",
    "#Problem 1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self,K):\n",
    "        self.g = 0\n",
    "        self.states = []  # record position and action taken at the position\n",
    "        self.actions = list(range(0,K)) #Set list of days as actions\n",
    "        self.lr = 0.2 #learning rate, takes longer when low\n",
    "        self.exp_rate = 0.01 #exploration rate .7\n",
    "        self.decay_gamma = 0.85 #Reward decay does not work well when 1 .85\n",
    "\n",
    "        self.switch = False\n",
    "\n",
    "        # initial Q values\n",
    "        self.Q_values = list(range(0,K))\n",
    "        for a in self.actions:\n",
    "            self.Q_values[a] = 0  # Q value is a dict of dict\n",
    "\n",
    "    def giveReward(self,x_k,b,x_k_z):\n",
    "        self.indiv = x_k*np.exp(-x_k/b)\n",
    "        \n",
    "        self.G = 0 \n",
    "        for x_k_g in x_k_z:\n",
    "            self.G = x_k_g*np.exp(-x_k_g/b) +self.G\n",
    "        #Add a difference reward here later\n",
    "        self.minusi = 0\n",
    "        self.diff = 0\n",
    "        #Get list for if agent didnt exist\n",
    "        self.x_k_2 = np.array(x_k_z)\n",
    "        self.x_k_2[self.action] = self.x_k_2[self.action] - 1\n",
    "        #Repeat reward calculation\n",
    "        for x_k_g2 in self.x_k_2:\n",
    "            self.minusi = x_k_g2*np.exp(-x_k_g2/b) + self.minusi\n",
    "        #Calculate difference reward\n",
    "        self.diff = self.G-self.minusi\n",
    "        #Now calculate \"average difference reward\"\n",
    "        days = len(self.actions)\n",
    "        self.x_k_2 = self.x_k_2 + 1/days\n",
    "        for x_k_g2 in self.x_k_2:\n",
    "            self.minusi = x_k_g2*np.exp(-x_k_g2/b) + self.minusi\n",
    "        self.diff2 = self.G-self.minusi\n",
    "        \n",
    "\n",
    "        return self.indiv,self.G,self.diff,self.diff2\n",
    "\n",
    "    def State(self):\n",
    "        pass\n",
    "\n",
    "    def chooseAction(self):\n",
    "        # choose action with most expected value\n",
    "        mx_nxt_reward = 0\n",
    "        action = \"\"\n",
    "\n",
    "        #Exploration action\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # greedy action\n",
    "            for a in self.actions:\n",
    "                nxt_reward = self.Q_values[a]\n",
    "                if nxt_reward >= mx_nxt_reward:\n",
    "                    action = a\n",
    "                    mx_nxt_reward = nxt_reward\n",
    "                \n",
    "        if action == \"\":    \n",
    "            action = np.random.choice(self.actions)\n",
    "            # print(\"current pos: {}, greedy aciton: {}\".format(self.State.state, action))\n",
    "        return action\n",
    "    \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.reward = 0\n",
    "        self.switch = False\n",
    "    \n",
    "    \n",
    "    def play(self, i,x_k,b,x_k_z,rtype):\n",
    "        if not self.switch:\n",
    "            # to the end of game back propagate reward\n",
    "            self.action = self.chooseAction()\n",
    "            # append trace\n",
    "            self.states.append(self.action) #Choose 1-6\n",
    "            #print(\"current position {} action {}\".format(self.State.state, action))\n",
    "            # mark is end\n",
    "            #self.State.isEndFunc()\n",
    "            self.switch = True\n",
    "            return(i,self)\n",
    "        else:\n",
    "        # back propagate\n",
    "            reward1,reward2,reward3,reward4 = self.giveReward(x_k,b,x_k_z)\n",
    "            rlist = [reward1,reward2,reward3,reward4]\n",
    "            reward = rlist[rtype]\n",
    "            self.Q_values[self.action] = reward\n",
    "            for s in reversed(self.states):\n",
    "                current_q_value = self.Q_values\n",
    "                reward = current_q_value + self.lr * (self.decay_gamma * reward - current_q_value)\n",
    "                self.Q_values = reward\n",
    "            self.reset()\n",
    "            i += 1\n",
    "            return(i,self)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    def run(N,b,k,cycles,reward_type):\n",
    "        #initialize agents\n",
    "        #N = 25\n",
    "        Agent_list = []\n",
    "        Action_list = [0]*N\n",
    "        \n",
    "        #b = 5\n",
    "        #k = 7\n",
    "        x_k = [0]*N\n",
    "        Count_list = [0]*k\n",
    "        Decision = [0]*k\n",
    "        #Create list of agent objects, each initialized\n",
    "        for i in range(N):\n",
    "            Agent_list.append(Agent(k))\n",
    "\n",
    "        #print(Agent_list[8].g) gives g\n",
    "        #Agent_list[8].indiv_reward(8,5)\n",
    "        #print(Agent_list[8].indiv)\n",
    "        i = np.zeros(N)\n",
    "        count = 0\n",
    "        max = 0\n",
    "        while count<1:\n",
    "            i = np.zeros(N)\n",
    "            while i[0]<cycles:\n",
    "                for x in range(N):\n",
    "                    i[x],Agent_list[x] = Agent_list[x].play(i[x],x_k[x],b,Count_list,reward_type)\n",
    "                    #Get info on actions here\n",
    "                    Action_list[x] = Agent_list[x].action\n",
    "                    #Count up all\n",
    "                    Count_list[Action_list[x]] = Count_list[Action_list[x]] + 1\n",
    "                for y in range(N):\n",
    "                    #Set all x_k values and run again \n",
    "                    x_k[y] = Count_list[Action_list[y]] #Number of people who attended same day\n",
    "                for z in range(N):\n",
    "                    #Now distribute rewards\n",
    "                    i[z],Agent_list[z] = Agent_list[z].play(i[z],x_k[z],b,Count_list,reward_type)\n",
    "                #Reset Count List?\n",
    "                Count_list = [0]*k\n",
    "            #print(Agent_list[0].Q_values)\n",
    "            for i in range(N):\n",
    "                #For each agent\n",
    "                #Get the index of the maximum q_value\n",
    "                index = np.argmax(Agent_list[i].Q_values)\n",
    "                Decision[index] = Decision[index] + 1 \n",
    "            count = count+1\n",
    "        return(Agent_list[0].G,Decision)\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0, 0, 50)\n",
      "(0, 0, 0, 0, 1, 49)\n",
      "(0, 0, 0, 0, 2, 48)\n",
      "(0, 0, 0, 0, 3, 47)\n",
      "(0, 0, 0, 0, 4, 46)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "trials = 200\n",
    "N = 50\n",
    "k = 6\n",
    "b = 4 \n",
    "score,decision = run(N,b,k,trials,reward_type = 0)\n",
    "\n",
    "\n",
    "# Generate all combinations of num_elements that sum to target_sum\n",
    "result = []\n",
    "    \n",
    "    # Use combinations_with_replacement to generate numbers that sum to target_sum\n",
    "for combination in itertools.combinations_with_replacement(range(N+1), k):\n",
    "        if sum(combination) == N:\n",
    "            result.append(combination)\n",
    "\n",
    "combinations = result\n",
    "# Print the first 5 results to check\n",
    "for comb in combinations[:5]:\n",
    "    print(comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 10 10 10 10 10]\n",
      "[8 8 8 8 9 9]\n"
     ]
    }
   ],
   "source": [
    "#Individual Reward  \n",
    "for comb in combinations:\n",
    "    decision = np.array(comb)\n",
    "    Nash = True\n",
    "    for agent_no,total in enumerate(decision):\n",
    "        Score = total*np.exp(-total/b)\n",
    "        #print(decision)\n",
    "        current_total = total\n",
    "        #print(decision_new)\n",
    "        #print(Score)\n",
    "        #print(agent_no)\n",
    "        if total == 0:\n",
    "            continue\n",
    "        decision[agent_no] = decision[agent_no] -1\n",
    "        for i in range(len(decision)):\n",
    "            #decision_new[i] = decision[i]+1\n",
    "            #Calc reward\n",
    "            New_decision_Total = decision[i]+1\n",
    "            Reward = New_decision_Total*np.exp(-New_decision_Total/b)\n",
    "            \n",
    "            if Reward>Score:\n",
    "                Nash = False\n",
    "                #print(agent_no)\n",
    "                #print(i)\n",
    "                #print(New_decision_Total)\n",
    "                #print(Score)\n",
    "                #print(Reward)\n",
    "        decision[agent_no] = decision[agent_no] +1\n",
    "    if Nash==True:\n",
    "        print(decision)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4  4  4  4  4 30]\n"
     ]
    }
   ],
   "source": [
    "#Global Reward \n",
    "decision_new = np.zeros(k) \n",
    "for comb in combinations:\n",
    "    decision = np.array(comb)\n",
    "    Nash = True\n",
    "    Score = 0\n",
    "    #Do global instead of individual\n",
    "    for agent,total_count in enumerate(decision):\n",
    "        Score = total_count*np.exp(-total_count/b) +Score\n",
    "    for agent_no,total in enumerate(decision):\n",
    "        #Score = total*np.exp(-total/b)\n",
    "        #print(decision)\n",
    "        current_total = total\n",
    "        #print(decision_new)\n",
    "        #print(Score)\n",
    "        #print(agent_no)\n",
    "        if total == 0:\n",
    "            continue\n",
    "            \n",
    "        decision[agent_no] = decision[agent_no] -1\n",
    "        for i in range(len(decision)):\n",
    "            decision_new = decision.copy()\n",
    "            #Calc reward\n",
    "            Total_count = decision[i].copy()\n",
    "            decision_new[i] = Total_count+1\n",
    "            #Reward = New_decision_Total*np.exp(-New_decision_Total/b)\n",
    "            Reward = 0\n",
    "            #Now calculate global reward\n",
    "            for agent_no_rew,total_count_2 in enumerate(decision_new):\n",
    "                Reward = total_count_2*np.exp(-total_count_2/b) +Reward\n",
    "                \n",
    "            #print(Reward)\n",
    "            #print(decision)\n",
    "            #print(Score)\n",
    "            #print(decision_new)\n",
    "            if Reward>Score:\n",
    "                #print(agent_no)\n",
    "                #print(comb)\n",
    "                Nash = False\n",
    "                #print(agent_no)\n",
    "                #print(i)\n",
    "                #print(decision)\n",
    "            \n",
    "                \n",
    "            \n",
    "        decision[agent_no] = decision[agent_no] +1\n",
    "        #print(Score)\n",
    "    if Nash==True:\n",
    "        print(decision)\n",
    "        \n",
    "        pass\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
